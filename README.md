
# Meta-OptimizaciÃ³n de Q-Learning: La Carrera a la PerfecciÃ³n

Este repositorio contiene un estudio avanzado de Aprendizaje por Refuerzo (Reinforcement Learning) aplicado al "Tres en Raya". El objetivo no es solo crear un agente inteligente, sino encontrar los parÃ¡metros de aprendizaje ($\alpha$ y $\gamma$) mÃ¡s eficientes mediante **Grid Search** y **Algoritmos GenÃ©ticos**, evaluÃ¡ndolos a travÃ©s de un sistema de **Rating Elo**.

**Materia:** Inteligencia Artificial  
**InstituciÃ³n:** Universidad Nacional Experimental de Guayana (UNEG)  
**Profesor:** Manuel Paniccia  
**Fecha de entrega:** 31 de Enero de 2026

---

## ğŸ”¬ DescripciÃ³n del Proyecto

A diferencia de un enfoque tradicional (Minimax), este proyecto implementa **Q-Learning Tabular** donde el agente aprende desde cero a travÃ©s de la experiencia. El nÃºcleo del proyecto es la comparaciÃ³n de agentes mediante un torneo genÃ©tico, para obtener los mejores hiperparÃ¡metros de entrenamiento.

---

## ğŸ› ï¸ TecnologÃ­as y Entorno

El proyecto estÃ¡ contenerizado para garantizar consistencia en cualquier entorno Linux (probado en Linux Mint / Fedora).

- **Lenguaje:** Python 3.12+
- **Gestor de Paquetes:** `uv` (Astral)
- **Interfaz GrÃ¡fica:** Pygame (para visualizaciÃ³n de duelos)
- **CÃ¡lculo MatemÃ¡tico:** Numpy
- **ContenerizaciÃ³n:** Podman + Podman Compose
- **Calidad de CÃ³digo:** Ruff
- **Persistencia:** Pickle (para almacenamiento de Q-Tables)

---

## ğŸš€ InstalaciÃ³n y EjecuciÃ³n

### Prerrequisitos
- Tener instalado `podman` y `podman-compose`.
- Tener `make` instalado.

### 1. ConfiguraciÃ³n Inicial
```bash
git clone https://github.com/V-enekoder/qlearning-tictactoe.git
cd qlearning-tictactoe
make build
```

### 2. Comandos del Makefile
- **`make run`**: Ejecuta la interfaz grÃ¡fica y el duelo de campeones.
- **`make train`**: Lanza el proceso de Grid Search y Algoritmo GenÃ©tico.
- **`make tournament`**: Ejecuta el torneo de Elo entre los agentes guardados.
- **`make shell`**: Acceso directo al contenedor.
- **`make stop`**: Detiene y limpia los contenedores.

---

## ğŸ“‚ Estructura del Proyecto

```text
/
â”œâ”€â”€ compose.yml           # ConfiguraciÃ³n de Podman y X11
â”œâ”€â”€ Dockerfile            # Imagen base (Python + dependencias SDL)
â”œâ”€â”€ pyproject.toml        # Dependencias (Numpy, Pygame, etc.)
â”œâ”€â”€ Makefile              # Atajos de ejecuciÃ³n
â”œâ”€â”€ models/               # Almacenamiento de Q-Tables (.pkl)
â””â”€â”€ src/
    â”œâ”€â”€ agent.py          # Clase QLearner y lÃ³gica de Bellman
    â”œâ”€â”€ minimax.py        # Agente perfecto para evaluaciÃ³n
    â”œâ”€â”€ genetic.py        # LÃ³gica del Algoritmo GenÃ©tico
    â”œâ”€â”€ dashboard.py      # GeneraciÃ³n de grÃ¡ficas de aprendizaje
    â””â”€â”€ main.py           # Interfaz visual y ejecuciÃ³n principal
```

---

## ğŸ“Š MÃ©tricas de EvaluaciÃ³n

El Ã©xito de los agentes se mide bajo tres criterios:
1.  **Punto de PerfecciÃ³n ($P_0$):** NÃºmero de partidas necesarias para dejar de perder contra Minimax.
2.  **Rating Elo:** Puntaje relativo de fuerza entre las distintas configuraciones de agentes.
3.  **Estabilidad de Convergencia:** Capacidad del agente para mantener el nivel Ã³ptimo tras alcanzarlo.

---

## ğŸ”§ SoluciÃ³n de Problemas Comunes

### 1. Error de Display (Pygame)
Si al ejecutar `make run` obtienes un error de video, asegÃºrate de habilitar el acceso a X11 en el host:
```bash
xhost +local:
```

### 2. SincronizaciÃ³n de dependencias
Si notas que faltan librerÃ­as dentro del contenedor tras actualizar el `pyproject.toml`:
```bash
podman exec -it qlearning_dev uv sync
```

### 3. Persistencia de Modelos
Los modelos se guardan en la carpeta `models/`. Si deseas resetear el entrenamiento, vacÃ­a esta carpeta antes de ejecutar el proceso de entrenamiento.
```bash
rm models/*.pkl
```
5.  **Comandos:** AÃ±adÃ­ `make train` y `make tournament` como sugerencia para separar el entrenamiento de la visualizaciÃ³n.

Â¡Mucho Ã©xito con este nuevo enfoque! Es un salto de calidad enorme respecto al Minimax tradicional.
